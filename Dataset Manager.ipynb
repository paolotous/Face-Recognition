{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fabio': 0, 'Paolo': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"train_data\"\n",
    "labels = os.listdir(data_path)\n",
    "categories = np.arange(len(labels))\n",
    "category_dict = dict(zip(labels, categories))\n",
    "\n",
    "print(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press y to use this image for our dataset, press any other key to pass\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n",
      "Image Saved\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "faceClassifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "print(\"Press y to use this image for our dataset, press any other key to pass\")\n",
    "\n",
    "#Accessing each folder in our training folder\n",
    "for label in labels:\n",
    "    imgs_path = os.path.join(data_path, label)\n",
    "    img_names = os.listdir(imgs_path)\n",
    "    \n",
    "    #Accessing each image in the folder\n",
    "    for img_name in img_names:\n",
    "            if img_name == \"Thumbs.db\":\n",
    "                continue\n",
    "            img_path = os.path.join(imgs_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            #Detecting face with haarcascade\n",
    "            faces = faceClassifier.detectMultiScale(gray)\n",
    "            \n",
    "            #Cropping the image and letting user decide whether the image should be used for training\n",
    "            #If yes then we resize the image and append the data and its label to our dataset\n",
    "            for x,y,w,h in faces:\n",
    "                cropped_face = gray[y: y+h, x: x+w]\n",
    "                cv2.imshow(label, cropped_face)\n",
    "                key = cv2.waitKey(0)\n",
    "                \n",
    "                if key == 121:\n",
    "                    cropped_face = cv2.resize(cropped_face, (50, 50))\n",
    "                    data.append(cropped_face)\n",
    "                    target.append(category_dict[label])\n",
    "                    print(\"Image Saved\")\n",
    "                elif key == 27:\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "            \n",
    "cv2.destroyAllWindows()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "faceClassifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for label in labels:\n",
    "    imgs_path = os.path.join(data_path, label)\n",
    "    img_names = os.listdir(imgs_path)\n",
    "    \n",
    "    for img_name in img_names:\n",
    "        if img_name == \"Thumbs.db\":\n",
    "            continue\n",
    "        img_path = os.path.join(imgs_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = faceClassifier.detectMultiScale(img)\n",
    "        \n",
    "        for x,y,w,h in faces:\n",
    "            cropped_face = img[y: y+h, x: x+w]\n",
    "            cropped_face = cv2.resize(cropped_face, (50, 50))\n",
    "            data.append(cropped_face)\n",
    "            target.append(category_dict[label])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "target = np.array(target)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "#Flattening the data\n",
    "numImages, height, width = data.shape\n",
    "data = data.reshape(numImages, height*width)\n",
    "\n",
    "#Saving the dataset\n",
    "np.save('data', data)\n",
    "np.save('target', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
